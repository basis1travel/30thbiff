import streamlit as st
import gspread
from gspread_dataframe import get_as_dataframe, set_with_dataframe
from google.oauth2.service_account import Credentials
import pandas as pd
from geopy.geocoders import Nominatim
import time
import pydeck as pdk
import re
import requests
from bs4 import BeautifulSoup

# --- Password Protection ---
def check_password():
    """Returns `True` if the user had the correct password."""
    def password_entered():
        """Checks whether a password entered by the user is correct."""
        if st.session_state["password"] == st.secrets["APP_PASSWORD"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]
        else:
            st.session_state["password_correct"] = False

    if "password_correct" not in st.session_state:
        st.text_input("Password", type="password", on_change=password_entered, key="password")
        return False
    elif not st.session_state["password_correct"]:
        st.text_input("Password", type="password", on_change=password_entered, key="password")
        st.error("ğŸ˜• Password incorrect")
        return False
    else:
        return True

# --- Google Sheets Connection & Data Handling ---
@st.cache_resource
def get_gspread_client():
    creds_dict = st.secrets["google_credentials"]["gcp"]
    scopes = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
    client = gspread.authorize(creds)
    return client

@st.cache_resource
def get_spreadsheet(_client):
    spreadsheet_key = st.secrets["google_credentials"]["spreadsheet_key"]
    spreadsheet = _client.open_by_key(spreadsheet_key)
    return spreadsheet

def create_sheet_if_not_exists(spreadsheet, sheet_name, headers):
    try:
        worksheet = spreadsheet.worksheet(sheet_name)
    except gspread.WorksheetNotFound:
        worksheet = spreadsheet.add_worksheet(title=sheet_name, rows="100", cols=len(headers) if headers else 20)
        if headers:
            worksheet.append_row(headers)
    return worksheet

@st.cache_data(ttl=600) # Cache data for 10 minutes
def load_data(_worksheet):
    return get_as_dataframe(_worksheet, usecols=None, header=0, dtype=str).fillna("")

def save_data(worksheet, df):
    worksheet.clear()
    set_with_dataframe(worksheet, df, include_index=False, resize=True)
    # Clear relevant caches after saving data
    st.cache_data.clear()


# --- Geocoding Function ---
@st.cache_data
def geocode_address(address, name):
    """
    Geocodes an address string to latitude and longitude.
    Falls back to name if address geocoding fails.
    """
    # 1st try: Address
    if address and not pd.isna(address) and str(address).strip():
        try:
            clean_address = address.split('(')[0].strip()
            query = f"ë¶€ì‚° {clean_address}"
            geolocator = Nominatim(user_agent="biff_planner_app")
            location = geolocator.geocode(query, timeout=10)
            time.sleep(1)
            if location:
                return location.latitude, location.longitude
        except Exception:
            pass # Fallback to name

    # 2nd try: Name
    if name and not pd.isna(name) and str(name).strip():
        try:
            clean_name = name.split('(')[0].strip()
            query = f"ë¶€ì‚° {clean_name}"
            geolocator = Nominatim(user_agent="biff_planner_app")
            location = geolocator.geocode(query, timeout=10)
            time.sleep(1)
            if location:
                return location.latitude, location.longitude
        except Exception:
            return None, None
            
    return None, None


# --- BIFF Movie Crawling Function ---
@st.cache_data
def fetch_movie_info(url):
    # ... (crawl_biff.pyì˜ í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´)
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers)
        response.encoding = 'utf-8'
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        title_tag = soup.select_one(".film_info_title .tit_h1")
        if title_tag and title_tag.find('small'):
            title_tag.find('small').decompose()
        title_kor = title_tag.text.strip() if title_tag else ""

        base_info = {
            "í•œêµ­ì–´ ì œëª©": title_kor,
            "ì˜ì–´ ì œëª©": soup.select_one(".film_info_title .film_tit_en").text.strip() if soup.select_one(".film_info_title .film_tit_en") else "",
            "ê°ë…": soup.select_one(".film_director .dir_name").text.strip() if soup.select_one(".film_director .dir_name") else "",
            "Program Note": soup.select_one(".film_synopsis .desc").text.strip() if soup.select_one(".film_synopsis .desc") else ""
        }
        
        spec_list = soup.select(".film_info.film_tit ul > li")
        base_info["êµ­ê°€"] = spec_list[0].text.replace("êµ­ê°€", "").strip() if len(spec_list) > 0 else ""
        base_info["ì œì‘ ì—°ë„"] = spec_list[1].text.replace("ì œì‘ì—°ë„", "").strip() if len(spec_list) > 1 else ""
        base_info["ëŸ¬ë‹íƒ€ì„"] = spec_list[2].text.replace("ëŸ¬ë‹íƒ€ì„", "").strip() if len(spec_list) > 2 else ""
        base_info["ìƒì˜í¬ë§·"] = spec_list[3].text.replace("ìƒì˜í¬ë§·", "").strip() if len(spec_list) > 3 else ""
        base_info["ì»¬ëŸ¬"] = spec_list[4].text.replace("ì»¬ëŸ¬", "").strip() if len(spec_list) > 4 else ""
        
        hashtags = [tag.text.strip() for tag in soup.select(".film_tit .keywords")]
        for i in range(3):
            base_info[f"í•´ì‹œíƒœê·¸{i+1}"] = hashtags[i] if i < len(hashtags) else ""

        final_data_list = []
        schedule_tags = soup.select(".pgv_schedule .pgv_sch_list")
        for schedule in schedule_tags:
            schedule_info = base_info.copy()
            schedule_info.update({
                "ì˜ˆë§¤ì½”ë“œ": re.sub(r'\D', '', schedule.select_one(".code").text),
                "ë‚ ì§œ": schedule.select_one(".date").text.replace("ë‚ ì§œ", "").strip(),
                "ì‹œê°„": schedule.select_one(".time").text.replace("ì‹œê°„", "").strip(),
                "ìƒì˜ê´€": schedule.select_one(".theater").text.replace("ìƒì˜ê´€", "").strip(),
                "ê¸°íƒ€": " ".join([tag.text.strip() for tag in schedule.select(".sch_grade > span") if tag.text.strip()]),
                "ì˜í™”í˜ì´ì§€": url
            })
            final_data_list.append(schedule_info)
        
        return final_data_list if final_data_list else [base_info]
    except Exception:
        return None

# --- Streamlit UI ---
st.set_page_config(page_title="ë¶€ì‚° ì»¤í”Œ ì—¬í–‰ í”Œë˜ë„ˆ", layout="wide")

if not check_password():
    st.stop()

st.title("ğŸ’˜ 30íšŒ BIFF 4ë°• 5ì¼ ì»¤í”Œ ì—¬í–‰ í”Œë˜ë„ˆ (Google Sheets ì—°ë™)")

try:
    gspread_client = get_gspread_client()
    spreadsheet = get_spreadsheet(gspread_client)

    # Define headers and get worksheets
    overview_headers = ["key", "value"]
    acc_headers = ["ìˆ™ì†Œëª…", "ìœ„ì¹˜", "ì˜ˆìƒ ë¹„ìš©", "ì¥ì ", "ì˜ˆì•½ë§í¬", "ìƒíƒœ"]
    act_headers = ["í™œë™ëª…", "ì¥ì†Œ", "ì˜ˆìƒ ë¹„ìš©", "ì†Œìš”ì‹œê°„", "ë©”ëª¨"]
    movies_headers = ["í•œêµ­ì–´ ì œëª©", "ì˜ì–´ ì œëª©", "ê°ë…", "êµ­ê°€", "ì œì‘ ì—°ë„", "ëŸ¬ë‹íƒ€ì„", "ìƒì˜í¬ë§·", "ì»¬ëŸ¬", "í•´ì‹œíƒœê·¸1", "í•´ì‹œíƒœê·¸2", "í•´ì‹œíƒœê·¸3", "ì˜ˆë§¤ì½”ë“œ", "ë‚ ì§œ", "ì‹œê°„", "ìƒì˜ê´€", "ê¸°íƒ€", "ì˜ˆë§¤ìš°ì„ ìˆœìœ„", "ì˜ˆë§¤ì„±ê³µì—¬ë¶€", "ì˜í™”í˜ì´ì§€", "ì˜í™”ì°¸ê³ ìë£Œ", "Program Note"]
    events_headers = [
        "No.", "ìƒí˜¸", "ì˜ˆì•½ê³„íš", "ë°©ë¬¸ì¼ì", "ë°©ë¬¸ìš”ì¼", "ì˜ˆì•½ì‹œê°„", "ë°©ë¬¸ì‹œê°„", "Schedule", "í”Œë«í¼", "ì¢…ë¥˜", "ìˆ ", "ì½œ/í”„", 
        "í¬ìŠ¤íŒ…ë§ˆê°ì¼ì", "ì›¹í˜ì´ì§€", "ì§€ì›ë‚´ì—­", "ì˜ˆì•½ê°€ëŠ¥ì¼ì‹œ", "ë°©ë¬¸ì „íŠ¹ì´ì‚¬í•­", "ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼", 
        "ì£¼ì†Œ", "ìœ„ì¹˜ì„¤ëª…", "ê¶Œì—­", "ì„¸ë¶€ê¶Œì—­", "ì£¼ë¬¸ë©”ë‰´", "ì§€ì›ë¹„ìš©", "ì¶”ê°€ë¹„ìš©", "ë°©ë¬¸í›„íŠ¹ì´ì‚¬í•­", "ë¿¡ì´ë³„ì ", "ë¿¡ì´ì½”ë©˜íŠ¸", "ìœì°¬ë³„ì ", "ìœì°¬ì½”ë©˜íŠ¸"
    ]
    
    ws_overview = create_sheet_if_not_exists(spreadsheet, "overview", overview_headers)
    ws_acc = create_sheet_if_not_exists(spreadsheet, "accommodation_candidates", acc_headers)
    ws_act = create_sheet_if_not_exists(spreadsheet, "activity_candidates", act_headers)
    ws_movies = create_sheet_if_not_exists(spreadsheet, "movies", movies_headers)
    ws_events = create_sheet_if_not_exists(spreadsheet, "events", events_headers)
    ws_2024 = create_sheet_if_not_exists(spreadsheet, "biff_2024", [])

    # Load data
    df_overview = load_data(ws_overview)
    df_acc = load_data(ws_acc)
    df_act = load_data(ws_act)
    df_movies = load_data(ws_movies)
    df_events = load_data(ws_events)
    df_2024 = load_data(ws_2024)

    # --- UI Tabs ---
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ì—¬í–‰ ê°œìš”", "ğŸ“ ê³„íš ë²„í¼", "ğŸ¬ ì˜í™” ëª©ë¡", "ğŸ—ºï¸ ì‘ë…„ ì—¬í–‰ ëŒì•„ë³´ê¸°", "ğŸ—“ï¸ ìƒì„¸ ì¼ì •", "âœ¨ ì²´í—˜ë‹¨"])

    with tab1:
        st.header("ğŸ“Œ ì—¬í–‰ ê°œìš”")
        if 'key' in df_overview.columns and 'value' in df_overview.columns:
            overview_data = dict(zip(df_overview['key'], df_overview['value']))
        else:
            overview_data = {}
        title = st.text_input("ì—¬í–‰ ì œëª©", value=overview_data.get("title", "ì œ30íšŒ ë¶€ì‚°êµ­ì œì˜í™”ì œ(BIFF) ì»¤í”Œ ì—¬í–‰"))
        purpose = st.text_input("ì—¬í–‰ ëª©ì ", value=overview_data.get("purpose", "BIFF ì˜í™” ê´€ëŒ, ë¶€ì‚° ê´€ê´‘ ë° ì»¤í”Œ ì—¬í–‰"))
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.text_input("ì—¬í–‰ ì‹œì‘ì¼", value=overview_data.get("start_date", "2025-09-18"), disabled=True)
        with col2:
            end_date = st.text_input("ì—¬í–‰ ì¢…ë£Œì¼", value=overview_data.get("end_date", "2025-09-23"), disabled=True)
        if st.button("ğŸ’¾ ì—¬í–‰ ê°œìš” ì €ì¥í•˜ê¸°", key="save_overview"):
            new_overview_data = {"title": title, "purpose": purpose, "start_date": start_date, "end_date": end_date}
            df_overview_new = pd.DataFrame(new_overview_data.items(), columns=['key', 'value'])
            save_data(ws_overview, df_overview_new)
            st.success("âœ… ì—¬í–‰ ê°œìš”ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

    with tab2:
        st.header("ğŸ“ ê³„íš ë²„í¼ (ì•„ì´ë””ì–´)")
        with st.expander("ğŸ’¡ ì—¬í–‰ ê°€ì´ë“œë¼ì¸ ë³´ê¸°", expanded=True):
            st.subheader("ğŸ“ ë¶€ì‚° ì§€ì—­ë³„ ì¤‘ìš”ë„ (Tier List)")
            st.markdown("""
            - **1í‹°ì–´**: ê´‘ì•ˆë¦¬, ì„¼í…€
            - **2í‹°ì–´**: ë¶€ì‚°ì—­, ì„œë©´, í•´ìš´ëŒ€
            - **3í‹°ì–´**: ë‚¨í¬ë™+ìê°ˆì¹˜, ë¯¸í¬, ì²­ì‚¬í¬, ì†¡ì •
            - **4í‹°ì–´**: ì†¡ë„, ê¸°ì¥ (ë¶€ì‚° ê°€ê¹ê±°ë‚˜, ì—­ ê·¼ì²˜ or ì„¼í…€ê°€ëŠ” ë²„ìŠ¤ê°€ ë§ì€ ê³³)
            - **5í‹°ì–´**: ë‹¤ëŒ€í¬, ì˜ë„(íƒœì¢…ëŒ€), ê¸ˆë ¨ì‚°(ë²”ì–´ì‚¬), ê¸°ì¥ (ë¶€ì‚° ë©€ê³  ì ‘ê·¼ì„± ë–¨ì–´ì§€ëŠ” ê³³)
            
            *5í‹°ì–´ë¡œ ê°ˆìˆ˜ë¡ ì˜í™”ì œì™€ í•¨ê»˜ ì¦ê¸°ë ¤ë©´ ì‹œê°„ê³¼ ì²´ë ¥ì„ ë” ë§ì´ ì¨ì•¼ í•©ë‹ˆë‹¤.*
            """)
            st.subheader("ğŸ½ï¸ ë§›ì§‘/ëª…ì†Œ íƒë°© ê°€ì´ë“œ")
            st.markdown("ë¶€ì‚° ì§€ì—­ ëª…ë¬¼ ë§›ì§‘, ì‹œì¥ ë¡œì»¬ ë§›ì§‘, ëª…ì†Œ/êµ¬ê²½ê±°ë¦¬ ë“±ì„ ì•„ë˜ 'í•˜ê³  ì‹¶ì€ ê²ƒë“¤'ì— í›„ë³´ë¡œ ì¶”ê°€í•˜ì—¬ ê³„íší•´ë³´ì„¸ìš”.")
        st.divider()
        st.subheader("ğŸ¨ ìˆ™ì†Œ ì˜ˆë¹„ í›„ë³´")
        df_acc_new = st.data_editor(df_acc, num_rows="dynamic", use_container_width=True, key="acc_editor")
        if st.button("ğŸ’¾ ìˆ™ì†Œ í›„ë³´ ì €ì¥í•˜ê¸°", key="save_acc"):
            save_data(ws_acc, df_acc_new)
            st.success("âœ… ìˆ™ì†Œ ì˜ˆë¹„ í›„ë³´ ëª©ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()
        st.divider()
        st.subheader("ğŸ“‹ í•˜ê³  ì‹¶ì€ ê²ƒë“¤ (ì—‘í‹°ë¹„í‹°)")
        df_act_new = st.data_editor(df_act, num_rows="dynamic", use_container_width=True, key="act_editor")
        if st.button("ğŸ’¾ í•˜ê³  ì‹¶ì€ ê²ƒë“¤ ì €ì¥í•˜ê¸°", key="save_act"):
            save_data(ws_act, df_act_new)
            st.success("âœ… í•˜ê³  ì‹¶ì€ ê²ƒë“¤ ëª©ë¡ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

    with tab3:
        st.header("ğŸ¬ ê´€ëŒ í¬ë§ ì˜í™” ë¦¬ìŠ¤íŠ¸")
        st.info("BIFF ì˜í™” ì •ë³´ í˜ì´ì§€ URLì„ ì…ë ¥í•˜ê³  'ì •ë³´ ê°€ì ¸ì˜¤ê¸°' ë²„íŠ¼ì„ ëˆ„ë¥´ë©´, ì•„ë˜ í‘œì— ìƒì˜ ì •ë³´ê°€ ìë™ìœ¼ë¡œ ì¶”ê°€ë©ë‹ˆë‹¤.")
        
        url = st.text_input("ì˜í™” ì •ë³´ í˜ì´ì§€ URLì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”:", key="movie_url")
        if st.button("ì •ë³´ ê°€ì ¸ì˜¤ê¸°", key="fetch_movie"):
            if url:
                with st.spinner("ì˜í™” ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” ì¤‘..."):
                    new_movie_data = fetch_movie_info(url)
                if new_movie_data:
                    new_df = pd.DataFrame(new_movie_data)
                    # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸° ì „ì— ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.new_movies_to_add = new_df.to_dict('records')
                    st.success(f"{len(new_movie_data)}ê°œì˜ ìƒì˜ ì¼ì •ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤! ì•„ë˜ í‘œì— ì„ì‹œë¡œ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ìµœì¢… ì €ì¥ì„ ìœ„í•´ 'ì˜í™” ëª©ë¡ ì €ì¥í•˜ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                else:
                    st.error("ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•˜ê±°ë‚˜ ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€í•  ì˜í™” ë°ì´í„°ê°€ ìˆìœ¼ë©´, í˜„ì¬ í‘œì— í•©ì³ì„œ ë³´ì—¬ì¤Œ
        if 'new_movies_to_add' in st.session_state:
            new_movies_df = pd.DataFrame(st.session_state.new_movies_to_add)
            display_df = pd.concat([df_movies, new_movies_df], ignore_index=True).fillna('')
        else:
            display_df = df_movies

        st.divider()
        st.subheader("ì „ì²´ ì˜í™” ëª©ë¡")
        df_movies_new = st.data_editor(display_df, num_rows="dynamic", use_container_width=True, key="movies_editor")
        
        if st.button("ğŸ’¾ ì˜í™” ëª©ë¡ ì €ì¥í•˜ê¸°", key="save_movies"):
            save_data(ws_movies, df_movies_new)
            # ì €ì¥ í›„ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
            if 'new_movies_to_add' in st.session_state:
                del st.session_state.new_movies_to_add
            st.success("âœ… ì˜í™” ëª©ë¡ì´ Google Sheetsì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

    with tab4:
        st.header("ğŸ“Š 2024ë…„ ì—¬í–‰ íšŒê³  ë° ë¶„ì„")
        if df_2024.empty or 'ìƒí˜¸' not in df_2024.columns:
            st.warning("ì‘ë…„ ì—¬í–‰ ë°ì´í„°ê°€ 'biff_2024' ì‹œíŠ¸ì— ì—†ê±°ë‚˜ í˜•ì‹ì´ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
        data24 = df_2024[df_2024['ìƒí˜¸'].notna() & (df_2024['ìƒí˜¸'] != '') & (~df_2024['ìƒí˜¸'].str.contains("Day", na=False))].copy()
        for col in ['ì§€ì›ë¹„ìš©', 'ì¶”ê°€ë¹„ìš©']:
            data24[col] = pd.to_numeric(data24[col], errors='coerce').fillna(0)
        data24['ì´ë¹„ìš©'] = data24['ì§€ì›ë¹„ìš©'] + data24['ì¶”ê°€ë¹„ìš©']
        
        # --- 1. í•µì‹¬ ì§€í‘œ (Key Metrics) ---
        st.subheader("ğŸ‘‘ í•œëˆˆì— ë³´ëŠ” ì‘ë…„ ì—¬í–‰")
        col1, col2, col3 = st.columns(3)
        col1.metric("ì´ ì§€ì¶œì•¡", f"{int(data24['ì´ë¹„ìš©'].sum()):,} ì›")
        col2.metric("ì‹¤ì œ ì§€ì¶œì•¡ (ë‚´ëˆë‚´ì‚°)", f"{int(data24['ì¶”ê°€ë¹„ìš©'].sum()):,} ì›")
        col3.metric("ì²´í—˜ë‹¨ ì§€ì› ê°€ì¹˜", f"{int(data24['ì§€ì›ë¹„ìš©'].sum()):,} ì›")

        # --- 2. ì§€ì¶œ ë¶„ì„ ---
        st.subheader("ğŸ’¸ ì§€ì¶œ ë¶„ì„")
        food_cats = ['ë¼ì§€', 'ìŠ¤ì‹œ/íšŒ', 'ë””ì €íŠ¸', 'ì†Œ', 'ì¹´í˜', 'ë³µì–´', 'ì™€ì¸ë°”', 'ìƒëŸ¬ë“œ/í¬ì¼€', 'ì´ìì¹´ì•¼']
        data24['ì¹´í…Œê³ ë¦¬'] = data24['ì¢…ë¥˜'].apply(lambda x: 'ì‹ìŒë£Œ' if x in food_cats else ('êµí†µ' if x == 'ì´ë™ìˆ˜ë‹¨' else ('ë¬¸í™”/ì˜ˆìˆ ' if x == 'ë¬¸í™”ì˜ˆìˆ ' else ('ìˆ™ì†Œ' if x == 'ìˆ™ì†Œ' else 'ê¸°íƒ€'))))
        spending_by_cat = data24.groupby('ì¹´í…Œê³ ë¦¬')['ì´ë¹„ìš©'].sum().sort_values(ascending=False)
        st.bar_chart(spending_by_cat)

        # --- 3. ë™ì„  ë° ì§€ì—­ ë¶„ì„ ---
        st.subheader("ğŸ—ºï¸ ë™ì„  ë° ì§€ì—­ ë¶„ì„")
        map_data = data24.copy()
        if 'lat' not in map_data.columns or 'lon' not in map_data.columns:
            map_data['lat'], map_data['lon'] = None, None
        
        rows_to_geocode = map_data[pd.to_numeric(map_data['lat'], errors='coerce').isna()]
        if not rows_to_geocode.empty:
            with st.spinner(f"{len(rows_to_geocode)}ê°œ ì¥ì†Œì˜ ì¢Œí‘œ ê³„ì‚° ì¤‘..."):
                for index, row in rows_to_geocode.iterrows():
                    lat, lon = geocode_address(row.get('ì£¼ì†Œ'), row.get('ìƒí˜¸'))
                    map_data.loc[index, 'lat'] = lat
                    map_data.loc[index, 'lon'] = lon
        
        map_data['lat'] = pd.to_numeric(map_data['lat'], errors='coerce')
        map_data['lon'] = pd.to_numeric(map_data['lon'], errors='coerce')
        st.map(map_data.dropna(subset=['lat', 'lon']), zoom=11)

        # --- 4. ì‹œê°„ ê´€ë¦¬ ë¶„ì„ ---
        st.subheader("â° ì‹œê°„ ê´€ë¦¬ ë¶„ì„")
        time_data = data24[['ì˜ˆì•½ì‹œê°„', 'ë°©ë¬¸ì‹œê°„']].copy()
        time_data = time_data[time_data['ì˜ˆì•½ì‹œê°„'].str.strip().ne('') & time_data['ë°©ë¬¸ì‹œê°„'].str.strip().ne('')]
        if not time_data.empty:
            time_data['ì˜ˆì•½ì‹œê°„_dt'] = pd.to_datetime(time_data['ì˜ˆì•½ì‹œê°„'], format='%H:%M', errors='coerce')
            time_data['ë°©ë¬¸ì‹œê°„_dt'] = pd.to_datetime(time_data['ë°©ë¬¸ì‹œê°„'], format='%H:%M', errors='coerce')
            time_data.dropna(inplace=True)
            time_data['ì°¨ì´(ë¶„)'] = (time_data['ë°©ë¬¸ì‹œê°„_dt'] - time_data['ì˜ˆì•½ì‹œê°„_dt']).dt.total_seconds() / 60
            avg_diff = time_data['ì°¨ì´(ë¶„)'].mean()
            st.metric("í‰ê·  ë„ì°© ì‹œê°„", f"{'ì˜ˆì•½ë³´ë‹¤ ' + str(int(abs(avg_diff))) + 'ë¶„ ì¼ì°' if avg_diff < 0 else 'ì˜ˆì•½ë³´ë‹¤ ' + str(int(avg_diff)) + 'ë¶„ ëŠ¦ê²Œ'}")



        # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
        data24 = df_2024[df_2024['ìƒí˜¸'].notna() & (df_2024['ìƒí˜¸'] != '') & (~df_2024['ìƒí˜¸'].str.contains("Day", na=False))].copy()
        for col in ['ì§€ì›ë¹„ìš©', 'ì¶”ê°€ë¹„ìš©']:
            data24[col] = pd.to_numeric(data24[col], errors='coerce').fillna(0)
        data24['ì´ë¹„ìš©'] = data24['ì§€ì›ë¹„ìš©'] + data24['ì¶”ê°€ë¹„ìš©']
        data24['ë°©ë¬¸ì¼ì'] = pd.to_datetime(data24['ë°©ë¬¸ì¼ì'], errors='coerce')
        data24['ë°©ë¬¸ì‹œê°„_dt'] = pd.to_datetime(data24['ë°©ë¬¸ì‹œê°„'], format='%H:%M', errors='coerce')
        data24.sort_values(by=['ë°©ë¬¸ì¼ì', 'ë°©ë¬¸ì‹œê°„_dt'], inplace=True)
        data24.dropna(subset=['ë°©ë¬¸ì¼ì'], inplace=True)

        # --- ë‚ ì§œ ì„ íƒê¸° ---
        unique_dates = sorted(data24['ë°©ë¬¸ì¼ì'].dt.date.unique())
        selected_date_str = st.selectbox("ë¶„ì„í•  ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”:", [d.strftime('%Y-%m-%d') for d in unique_dates])
        selected_date = pd.to_datetime(selected_date_str).date()

        day_df = data24[data24['ë°©ë¬¸ì¼ì'].dt.date == selected_date].copy()
        day_df.reset_index(drop=True, inplace=True)

        # --- ì¢Œí‘œ ê³„ì‚° ---
        if 'lat' not in day_df.columns or 'lon' not in day_df.columns:
            day_df['lat'], day_df['lon'] = None, None
        
        rows_to_geocode = day_df[pd.to_numeric(day_df['lat'], errors='coerce').isna()]
        if not rows_to_geocode.empty:
            with st.spinner(f"{len(rows_to_geocode)}ê°œ ì¥ì†Œì˜ ì¢Œí‘œ ê³„ì‚° ì¤‘..."):
                for index, row in rows_to_geocode.iterrows():
                    lat, lon = geocode_address(row.get('ì£¼ì†Œ'), row.get('ìƒí˜¸'))
                    day_df.loc[index, 'lat'] = lat
                    day_df.loc[index, 'lon'] = lon
        
        day_df['lat'] = pd.to_numeric(day_df['lat'], errors='coerce')
        day_df['lon'] = pd.to_numeric(day_df['lon'], errors='coerce')
        map_data = day_df.dropna(subset=['lat', 'lon']).copy()
        map_data.reset_index(drop=True, inplace=True)

        if map_data.empty:
            st.warning("ì„ íƒí•œ ë‚ ì§œì— ì§€ë„ì— í‘œì‹œí•  ì¥ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")        

            # --- Pydeck ì‹œê°í™” ---
        st.subheader(f"ğŸ—ºï¸ {selected_date_str} ì´ë™ ê²½ë¡œ")

        # 1. ê²½ë¡œ ì„  ë ˆì´ì–´ (ì‹œê°„ì— ë”°ë¥¸ ìƒ‰ìƒ ë³€í™”)
        path_data = []
        for i in range(len(map_data) - 1):
            path_data.append({
                "start": [map_data.loc[i, 'lon'], map_data.loc[i, 'lat']],
                "end": [map_data.loc[i + 1, 'lon'], map_data.loc[i + 1, 'lat']],
                "color": [255, 0, 0, 255 - (i * (200 / len(map_data)))], # ì ì  ì˜…ì–´ì§€ëŠ” ë¶‰ì€ìƒ‰
                "tooltip": f"{i+1}. {map_data.loc[i, 'ìƒí˜¸']} -> {i+2}. {map_data.loc[i+1, 'ìƒí˜¸']}"
            })
        
        path_layer = pdk.Layer(
            "LineLayer",
            data=path_data,
            get_source_position="start",
            get_target_position="end",
            get_color="color",
            get_width=5,
            highlight_color=[255, 255, 0],
            picking_radius=10,
            auto_highlight=True,
        )

        # 2. ë¹„ìš© ê¸°ë°˜ ì› ë ˆì´ì–´
        scatter_layer = pdk.Layer(
            "ScatterplotLayer",
            data=map_data,
            get_position=["lon", "lat"],
            get_radius="ì´ë¹„ìš© * 0.2 + 50", # ë¹„ìš©ì— ë”°ë¼ ì› í¬ê¸° ì¡°ì ˆ
            get_fill_color=[30, 144, 255, 180], # íŒŒë€ìƒ‰ ì›
            pickable=True,
        )

        # 3. ìˆœì„œ ì•„ì´ì½˜ ë ˆì´ì–´
        icon_data = []
        for i, row in map_data.iterrows():
            icon_data.append({
                "coordinates": [row['lon'], row['lat']],
                "text": str(i + 1),
                "tooltip": f"**{i+1}. {row['ìƒí˜¸']}**\n- ì¢…ë¥˜: {row['ì¢…ë¥˜']}\n- ì´ë¹„ìš©: {int(row['ì´ë¹„ìš©']):,}ì›"
            })

        icon_layer = pdk.Layer(
            "TextLayer",
            data=icon_data,
            get_position="coordinates",
            get_text="text",
            get_size=20,
            get_color=[255, 255, 255],
            get_angle=0,
            get_text_anchor="'middle'",
            get_alignment_baseline="'center'",
        )

        # ì§€ë„ ì´ˆê¸° ì‹œì  ì„¤ì •
        view_state = pdk.ViewState(
            latitude=map_data["lat"].mean(),
            longitude=map_data["lon"].mean(),
            zoom=12,
            pitch=45,
        )

        # ë± ë Œë”ë§
        r = pdk.Deck(
            layers=[scatter_layer, path_layer, icon_layer],
            initial_view_state=view_state,
            map_style="mapbox://styles/mapbox/light-v10",
            tooltip={"html": "{tooltip}", "style": {"color": "white"}},
        )
        st.pydeck_chart(r)

        # --- ê²½ë¡œ ì •ë³´ ìš”ì•½ ---
        st.subheader("ğŸ“ ê²½ë¡œ ì •ë³´")
        for i, row in map_data.iterrows():
            st.markdown(f"**{i+1}. {row['ìƒí˜¸']}** ({row['ë°©ë¬¸ì‹œê°„']}) - {int(row['ì´ë¹„ìš©']):,}ì›")

    with tab5:
        st.header("ğŸ—“ï¸ ìƒì„¸ ì¼ì •")
        st.info("ìƒì„¸ ì¼ì •ì€ Google Sheetsì—ì„œ ì§ì ‘ í¸ì§‘í•˜ëŠ” ê²ƒì´ ë” í¸ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    with tab6:
        st.header("âœ¨ ì²´í—˜ë‹¨ ì •ë³´")
        df_events_new = st.data_editor(
            df_events, num_rows="dynamic", use_container_width=True, key="events_editor",
            column_config={"ì›¹í˜ì´ì§€": st.column_config.LinkColumn("ì›¹í˜ì´ì§€")}
        )
        if st.button("ğŸ’¾ ì²´í—˜ë‹¨ ì •ë³´ ì €ì¥í•˜ê¸°", key="save_events"):
            save_data(ws_events, df_events_new)
            st.success("âœ… ì²´í—˜ë‹¨ ì •ë³´ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

except Exception as e:
    st.error(f"ì•± ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")